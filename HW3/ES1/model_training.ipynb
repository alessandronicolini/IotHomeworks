{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "superb-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import zlib\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--version', type=str, required=True, help='model version')\n",
    "args = parser.parse_args()\n",
    "version = args.version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "posted-lloyd",
   "metadata": {},
   "source": [
    "args = {\"version\" : \"little\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "northern-stephen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: git: comando non trovato\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 24, 8, 128)        1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 24, 8, 128)        512       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 24, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_9 (Depthwis (None, 22, 6, 128)        1152      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 22, 6, 64)         8192      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 22, 6, 64)         256       \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 22, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_10 (Depthwi (None, 20, 4, 64)         576       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 20, 4, 32)         2048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 20, 4, 32)         128       \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 20, 4, 32)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_11 (Depthwi (None, 18, 2, 32)         288       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 18, 2, 32)         1024      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 18, 2, 32)         128       \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 18, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 15,720\n",
      "Trainable params: 15,208\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 61s 173ms/step - loss: 1.8288 - sparse_categorical_accuracy: 0.3508 - val_loss: 1.1208 - val_sparse_categorical_accuracy: 0.6900\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 1.0407 - sparse_categorical_accuracy: 0.7467 - val_loss: 0.6972 - val_sparse_categorical_accuracy: 0.8288\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.6575 - sparse_categorical_accuracy: 0.8474 - val_loss: 0.5247 - val_sparse_categorical_accuracy: 0.8625\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.4899 - sparse_categorical_accuracy: 0.8816 - val_loss: 0.3973 - val_sparse_categorical_accuracy: 0.8988\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.3858 - sparse_categorical_accuracy: 0.9062 - val_loss: 0.3698 - val_sparse_categorical_accuracy: 0.8913\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.3257 - sparse_categorical_accuracy: 0.9158 - val_loss: 0.3016 - val_sparse_categorical_accuracy: 0.9112\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.2878 - sparse_categorical_accuracy: 0.9224 - val_loss: 0.3351 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9375 - val_loss: 0.2877 - val_sparse_categorical_accuracy: 0.9062\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.2272 - sparse_categorical_accuracy: 0.9433 - val_loss: 0.2715 - val_sparse_categorical_accuracy: 0.9200\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.2029 - sparse_categorical_accuracy: 0.9526 - val_loss: 0.2629 - val_sparse_categorical_accuracy: 0.9162\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.1845 - sparse_categorical_accuracy: 0.9559 - val_loss: 0.2661 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1675 - sparse_categorical_accuracy: 0.9615 - val_loss: 0.2493 - val_sparse_categorical_accuracy: 0.9212\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.1506 - sparse_categorical_accuracy: 0.9698 - val_loss: 0.2424 - val_sparse_categorical_accuracy: 0.9225\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.1516 - sparse_categorical_accuracy: 0.9695 - val_loss: 0.2380 - val_sparse_categorical_accuracy: 0.9300\n",
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.1440 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.2325 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.1243 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2495 - val_sparse_categorical_accuracy: 0.9150\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.1360 - sparse_categorical_accuracy: 0.9771 - val_loss: 0.2325 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9784 - val_loss: 0.2408 - val_sparse_categorical_accuracy: 0.9225\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.1370 - sparse_categorical_accuracy: 0.9741 - val_loss: 0.2469 - val_sparse_categorical_accuracy: 0.9275\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.1367 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.2374 - val_sparse_categorical_accuracy: 0.9225\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.1326 - sparse_categorical_accuracy: 0.9744 - val_loss: 0.2341 - val_sparse_categorical_accuracy: 0.9212\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.1324 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.2396 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.1373 - sparse_categorical_accuracy: 0.9748 - val_loss: 0.2326 - val_sparse_categorical_accuracy: 0.9250\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.2573 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.1314 - sparse_categorical_accuracy: 0.9782 - val_loss: 0.2368 - val_sparse_categorical_accuracy: 0.9237\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1265 - sparse_categorical_accuracy: 0.9791 - val_loss: 0.2527 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.9760 - val_loss: 0.2400 - val_sparse_categorical_accuracy: 0.9262\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1284 - sparse_categorical_accuracy: 0.9774 - val_loss: 0.2494 - val_sparse_categorical_accuracy: 0.9175\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.1230 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2364 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.2427 - val_sparse_categorical_accuracy: 0.9200\n",
      "25/25 [==============================] - 7s 266ms/step - loss: 0.2901 - sparse_categorical_accuracy: 0.9112\n",
      "WEIGHTS CLUSTERING\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.2393 - sparse_categorical_accuracy: 0.9274 - val_loss: 0.2588 - val_sparse_categorical_accuracy: 0.9137\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1586 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.2526 - val_sparse_categorical_accuracy: 0.9125\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1388 - sparse_categorical_accuracy: 0.9607 - val_loss: 0.2601 - val_sparse_categorical_accuracy: 0.9137\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpa4gywk05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa4gywk05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustered accuracy: 0.9062 %\n",
      "clustered size: 26.4414 kB\n",
      "\n",
      "POST TRAINING QUANTIZATION\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp80o0s4kn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp80o0s4kn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of quantized model: 0.9050 %\n",
      "size of quantized model: 20.6396 kB\n"
     ]
    }
   ],
   "source": [
    "version = \"little\"\n",
    "#***************************** CLASSES AND FUNCTION ****************************\n",
    "# DATASET CLASS ----------------------------------------------------------------\n",
    "class SignalGenerator:\n",
    "    def __init__(self, labels, sampling_rate, frame_length, frame_step,\n",
    "            num_mel_bins=None, lower_frequency=None, upper_frequency=None,\n",
    "            num_coefficients=None, mfcc=False):\n",
    "        self.labels = labels\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "        self.lower_frequency = lower_frequency\n",
    "        self.upper_frequency = upper_frequency\n",
    "        self.num_coefficients = num_coefficients\n",
    "        num_spectrogram_bins = (frame_length) // 2 + 1\n",
    "\n",
    "        if mfcc is True:\n",
    "            self.linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "                    self.num_mel_bins, num_spectrogram_bins, self.sampling_rate,\n",
    "                    self.lower_frequency, self.upper_frequency)\n",
    "            self.preprocess = self.preprocess_with_mfcc\n",
    "        else:\n",
    "            self.preprocess = self.preprocess_with_stft\n",
    "\n",
    "    def read(self, file_path):\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        label = parts[-2]\n",
    "        label_id = tf.argmax(label == self.labels)\n",
    "        audio_binary = tf.io.read_file(file_path)\n",
    "        audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "        audio = tf.squeeze(audio, axis=1)\n",
    "\n",
    "        return audio, label_id\n",
    "\n",
    "    def pad(self, audio):\n",
    "        zero_padding = tf.zeros([self.sampling_rate] - tf.shape(audio), dtype=tf.float32)\n",
    "        audio = tf.concat([audio, zero_padding], 0)\n",
    "        audio.set_shape([self.sampling_rate])\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def get_spectrogram(self, audio):\n",
    "        stft = tf.signal.stft(audio, frame_length=self.frame_length,\n",
    "                frame_step=self.frame_step, fft_length=self.frame_length)\n",
    "        spectrogram = tf.abs(stft)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    def get_mfccs(self, spectrogram):\n",
    "        mel_spectrogram = tf.tensordot(spectrogram,\n",
    "                self.linear_to_mel_weight_matrix, 1)\n",
    "        log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "        mfccs = mfccs[..., :self.num_coefficients]\n",
    "\n",
    "        return mfccs\n",
    "\n",
    "    def preprocess_with_stft(self, file_path):\n",
    "        audio, label = self.read(file_path)\n",
    "        audio = self.pad(audio)\n",
    "        spectrogram = self.get_spectrogram(audio)\n",
    "        spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "        spectrogram = tf.image.resize(spectrogram, [32, 32])\n",
    "\n",
    "        return spectrogram, label\n",
    "\n",
    "    def preprocess_with_mfcc(self, file_path):\n",
    "        audio, label = self.read(file_path)\n",
    "        audio = self.pad(audio)\n",
    "        spectrogram = self.get_spectrogram(audio)\n",
    "        mfccs = self.get_mfccs(spectrogram)\n",
    "        mfccs = tf.expand_dims(mfccs, -1)\n",
    "\n",
    "        return mfccs, label\n",
    "\n",
    "    def make_dataset(self, files, train):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "        ds = ds.map(self.preprocess, num_parallel_calls=4)\n",
    "        ds = ds.batch(32)\n",
    "        ds = ds.cache()\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "        return ds\n",
    "\n",
    "# SAVE MODELS-------------------------------------------------------------------\n",
    "def save_model(model, out_path):\n",
    "    \n",
    "    # .tflite format\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    model = converter.convert()\n",
    "\n",
    "    # compressed file\n",
    "    compressed_model = zlib.compress(model)\n",
    "        \n",
    "    # .zip format   \n",
    "    with open(out_path+'.tflite.zlib', 'wb') as f:\n",
    "        f.write(compressed_model)\n",
    "\n",
    "    return os.path.getsize(out_path+'.tflite.zlib')/1024\n",
    "\n",
    "# WEIGHTS CLUSTERING -----------------------------------------------------------\n",
    "def weights_clustering(\n",
    "    in_model, \n",
    "    out_path, \n",
    "    epochs=5, \n",
    "    lr=1e-3, \n",
    "    n_clusters=4, \n",
    "    only_dense=True):\n",
    "    \n",
    "    if only_dense:\n",
    "      def apply_clustering_to_dense(layer):\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "          return tfmot.clustering.keras.cluster_weights(\n",
    "              layer,\n",
    "              number_of_clusters=n_clusters,\n",
    "              cluster_centroids_init=tfmot.clustering.keras.CentroidInitialization.LINEAR\n",
    "              )\n",
    "        return layer\n",
    "\n",
    "      clustered_model = tf.keras.models.clone_model(\n",
    "          in_model, \n",
    "          clone_function = apply_clustering_to_dense)\n",
    "\n",
    "    else:     \n",
    "      # create model for weight clustering\n",
    "      clustered_model = tfmot.clustering.keras.cluster_weights(\n",
    "          in_model,\n",
    "          number_of_clusters=n_clusters,\n",
    "          cluster_centroids_init=tfmot.clustering.keras.CentroidInitialization.LINEAR\n",
    "          )\n",
    "    \n",
    "    # compile model\n",
    "    clustered_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "        )\n",
    "    \n",
    "    # fit model\n",
    "    clustered_model.fit(\n",
    "        train_ds,\n",
    "        batch_size=32, \n",
    "        epochs=epochs, \n",
    "        validation_data=val_ds)\n",
    "    \n",
    "    # clustered model evaluation\n",
    "    _, accuracy = clustered_model.evaluate(test_ds, verbose=0)\n",
    "    \n",
    "    # prepare for export and save\n",
    "    model_for_export = tfmot.clustering.keras.strip_clustering(clustered_model)\n",
    "    zip_size = save_model(model_for_export, out_path)\n",
    "    \n",
    "    return model_for_export, accuracy, zip_size\n",
    "\n",
    "# POST TRAINING QUANTIZATION ---------------------------------------------------\n",
    "def get_accuracy(interpreter, test_dataset):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    running_corrects = 0\n",
    "    total_elements = 0\n",
    "\n",
    "    for (batch, labels) in test_dataset:\n",
    "        total_elements += len(batch)\n",
    "        for test_sample, label in  zip(batch, labels):\n",
    "            test_sample = np.expand_dims(test_sample, axis=0).astype(np.float32)\n",
    "            interpreter.set_tensor(input_index, test_sample)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_index)\n",
    "            pred = np.argmax(output)\n",
    "            if pred == label:\n",
    "              running_corrects += 1\n",
    "  \n",
    "    return running_corrects/total_elements\n",
    "\n",
    "def representative_dataset():\n",
    "  train_set = train_ds.unbatch().take(200)\n",
    "  img_list = []\n",
    "  for image in train_set:\n",
    "    img_list.append(image[0].numpy())\n",
    "  img_arr = np.array(img_list)\n",
    "\n",
    "  for data in tf.data.Dataset.from_tensor_slices(img_arr).batch(1).take(100):\n",
    "    yield [data]\n",
    "\n",
    "def pt_quantization(in_model, out_path, q_type='float16'):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(in_model)\n",
    "    if q_type == 'float16':\n",
    "      converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "      converter.target_spec.supported_types = [tf.float16]\n",
    "    elif q_type == 'int8int16':\n",
    "      converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "      converter.representative_dataset = representative_dataset\n",
    "      converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n",
    "    else:\n",
    "      converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "      converter.representative_dataset = representative_dataset\n",
    "      \n",
    "    quant_tflite_model = converter.convert()\n",
    "\n",
    "    # compressed file\n",
    "    compressed_model = zlib.compress(quant_tflite_model)\n",
    "\n",
    "    # generate uncompressed version for latency test\n",
    "    with open(\"./little_model.tflite\", 'wb') as f:\n",
    "      f.write(quant_tflite_model)\n",
    "\n",
    "    # generate compressed version    \n",
    "    with open(out_path+'.tflite.zlib', 'wb') as f:\n",
    "        f.write(compressed_model)\n",
    "    \n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_content=quant_tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    accuracy = get_accuracy(interpreter, test_ds)\n",
    "    zip_size = os.path.getsize(out_path+'.tflite.zlib')/1024\n",
    "    \n",
    "    return accuracy, zip_size\n",
    "\n",
    "\n",
    "# MODEL FUNCTION ---------------------------------------------------------------\n",
    "def get_model(conv1, conv2, conv3, conv4):\n",
    "    model = models.Sequential([\n",
    "      layers.Input(shape=shape),\n",
    "      layers.Conv2D(filters=conv1, kernel_size=[3,3], strides=[2,1], use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1),\n",
    "      layers.ReLU(),\n",
    "      layers.DepthwiseConv2D(kernel_size=[3,3], strides=[1,1], use_bias=False),\n",
    "      layers.Conv2D(filters=conv2, kernel_size=[1,1], strides=[1,1], use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1),\n",
    "      layers.ReLU(),\n",
    "      layers.DepthwiseConv2D(kernel_size=[3,3], strides=[1,1], use_bias=False),\n",
    "      layers.Conv2D(filters=conv3, kernel_size=[1,1], strides=[1,1], use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1),\n",
    "      layers.ReLU(),\n",
    "      layers.DepthwiseConv2D(kernel_size=[3,3], strides=[1,1], use_bias=False),\n",
    "      layers.Conv2D(filters=conv4, kernel_size=[1,1], strides=[1,1], use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1),\n",
    "      layers.ReLU(),\n",
    "      layers.GlobalAveragePooling2D(),\n",
    "      layers.Dense(8)\n",
    "    ])\n",
    "    return model \n",
    "\n",
    "#********************************** MAIN BODY *****************************************\n",
    "# DATASET PREPARATION -----------------------------------------------------------------\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "github_dir = pathlib.Path('./IotHomeworks')\n",
    "if not github_dir.exists():\n",
    "  !git clone https://github.com/alessandronicolini/IotHomeworks.git\n",
    "code_path = \"./IotHomeworks/HW2\"\n",
    "\n",
    "data_dir = pathlib.Path('data/mini_speech_commands')\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')\n",
    "  \n",
    "#lista di labels\n",
    "labels=[]\n",
    "for el in os.listdir(\"./data/mini_speech_commands\"):\n",
    "  if el!=\"README.md\":\n",
    "    labels.append(el)\n",
    "\n",
    "#lista di training\n",
    "training_list=[]\n",
    "file=open(\"kws_train_split.txt\")\n",
    "for line in file:\n",
    "  training_list.append('.'+line[1:-1])\n",
    "\n",
    "#lista di validation\n",
    "validation_list=[]\n",
    "file=open(\"kws_val_split.txt\")\n",
    "for line in file:\n",
    "  validation_list.append('.'+line[1:-1])\n",
    "\n",
    "#lista di test\n",
    "test_list=[]\n",
    "file=open(\"kws_test_split.txt\")\n",
    "for line in file:\n",
    "  test_list.append('.'+line[1:-1])\n",
    "\n",
    "tot=len(training_list)+len(validation_list)+len(test_list)\n",
    "\n",
    "if version == \"little\":\n",
    "  # original shape 49,10,1, ok for case a\n",
    "  MFCC_OPTIONS = {'frame_length': 640, 'frame_step': 320, 'mfcc': True,'lower_frequency': 20, 'upper_frequency': 4000, 'num_mel_bins': 40,'num_coefficients': 10}\n",
    "  shape = [49, 10, 1]\n",
    "\n",
    "  model_options = {'conv1':128, 'conv2':64, 'conv3':32, 'conv4':32}\n",
    "\n",
    "  ws_options = {'out_path':'./'+version+'_clustered', \n",
    "                'epochs':3, \n",
    "                'lr':1e-4, \n",
    "                'n_clusters':12, \n",
    "                'only_dense':False}\n",
    "\n",
    "  ptq_options = {'out_path':'./'+str(version),\n",
    "                'q_type': 'else'}\n",
    "\n",
    "elif version == \"big\":\n",
    "  MFCC_OPTIONS = {'frame_length': 640, 'frame_step': 320, 'mfcc': True,'lower_frequency': 20, 'upper_frequency': 4000, 'num_mel_bins': 40,'num_coefficients': 10}\n",
    "  shape = [49, 10, 1]\n",
    "\n",
    "  model_options = {'conv1':512, 'conv2':512, 'conv3':512, 'conv4':256}\n",
    "\n",
    "                   \n",
    "generator = SignalGenerator(labels, 16000, **MFCC_OPTIONS)     \n",
    "train_ds = generator.make_dataset(training_list, True)\n",
    "val_ds = generator.make_dataset(validation_list, False)\n",
    "test_ds = generator.make_dataset(test_list, False)\n",
    "\n",
    "n_batches = 200\n",
    "\n",
    "# MODEL TRAINING ----------------------------------------------------------------------\n",
    "model = get_model(**model_options)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# learning rate scheduler\n",
    "learning_rate_fn = PolynomialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=3000,\n",
    "    end_learning_rate=1e-5\n",
    "    )\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_fn),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    ")\n",
    "\n",
    "# callbacks\n",
    "ckp_dir = \"./checkpoint/\"\n",
    "try:\n",
    "  os.mkdir(ckp_dir)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckp_dir, \n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='max', \n",
    "    save_freq='epoch')\n",
    "\n",
    "# fit model\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "# TEST AND FP32 FILE CREATION --------------------------------------------------\n",
    "# load and evaluate the best model\n",
    "base_model = tf.keras.models.load_model(ckp_dir)\n",
    "base_model.evaluate(test_ds, batch_size=32)\n",
    "if version == \"little\":\n",
    "\n",
    "    # OPTIMIZATION -----------------------------------------------------------------\n",
    "    print(\"WEIGHTS CLUSTERING\")\n",
    "    clustered_model, clustered_acc, clustered_size = weights_clustering(\n",
    "        in_model=base_model, \n",
    "        **ws_options)\n",
    "    print('clustered accuracy: %.4f %%'%clustered_acc)\n",
    "    print('clustered size: %.4f kB'%clustered_size)\n",
    "    print()\n",
    "\n",
    "    # Post training quantization\n",
    "    print('POST TRAINING QUANTIZATION')\n",
    "    quant_acc, quant_size = pt_quantization(in_model=clustered_model, \n",
    "                                            **ptq_options)\n",
    "    print('accuracy of quantized model: %.4f %%'%quant_acc)\n",
    "    print('size of quantized model: %.4f kB'%quant_size)\n",
    "    \n",
    "elif version ==\"big\":\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open('big_model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
